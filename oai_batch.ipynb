{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create jsonl file with requests for batch completion\n",
    "\n",
    "import json\n",
    "import tiktoken\n",
    "import os\n",
    "from generate_stories import create_simple_story_prompt, iterate_params\n",
    "\n",
    "MODEL_PARAMETERS = {\"top_p\": 0.9}\n",
    "\n",
    "def get_batch_dataset(num_completions, model, offset=0):\n",
    "    \n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    params_iterator = iterate_params()\n",
    "    for _ in range(offset):\n",
    "        next(params_iterator)\n",
    "    \n",
    "    while True:\n",
    "        lines = []\n",
    "        total_tokens = 0\n",
    "        for i in range(num_completions):\n",
    "            params = next(params_iterator)\n",
    "            prompt, num_stories_in_completion = create_simple_story_prompt(params.copy())\n",
    "            message_tokens = len(enc.encode(prompt))\n",
    "            total_tokens += message_tokens\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "            lines.append(json.dumps(\n",
    "                {\"custom_id\": str(i)+json.dumps(params | {\"expected_num_stories_in_completion\": num_stories_in_completion} | MODEL_PARAMETERS),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\"model\": model, \"messages\": messages, **MODEL_PARAMETERS}}\n",
    "                ))\n",
    "        \n",
    "        print(f\"Total input tokens: {total_tokens}\")\n",
    "\n",
    "        yield lines\n",
    "\n",
    "def write_batch_completion_file(num_completions, model, base_filename, offset=0):\n",
    "    iterator = get_batch_dataset(num_completions, model, offset)\n",
    "    counter = 1\n",
    "    filename = base_filename\n",
    "    while True:\n",
    "        lines = next(iterator)\n",
    "        filename = f\"{base_filename}_{str(counter)}.jsonl\"\n",
    "        with open(filename, \"w\") as fp:\n",
    "            fp.write(\"\\n\".join(lines))\n",
    "        counter += 1\n",
    "        yield filename\n",
    "    \n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Generation:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7233408 combinations...\n",
      "31 23\n",
      "Total input tokens: 1135\n",
      "Batch status: validating\n",
      "The input file is being validated. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: in_progress\n",
      "The batch is currently being processed. Please wait...\n",
      "Batch status: completed\n",
      "The batch is complete, downloading the results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Generation: 100%|██████████| 5/5 [37:26<00:00, 449.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# 2. Execute Batch Jobs\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "        \n",
    "NUM_COMPLETIONS = 5\n",
    "NUM_COMPLETIONS_PER_REQUEST = 5 # Calculate this based on rate limits, to be checked at https://platform.openai.com/settings/organization/limits\n",
    "OFFSET = 4\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY_SIMPLESTORIES\"])\n",
    "\n",
    "def check_batch_status(batch_id, batch_number, directory):\n",
    "    while True:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "\n",
    "        status = batch_status.status\n",
    "        print(f\"Batch status: {status}\")\n",
    "\n",
    "        if status == \"validating\":\n",
    "            print(\"The input file is being validated. Please wait...\")\n",
    "        elif status == \"failed\":\n",
    "            print(\"The input file has failed validation.\")\n",
    "            return False\n",
    "        elif status == \"in_progress\":\n",
    "            print(\"The batch is currently being processed. Please wait...\")\n",
    "        elif status == \"finalizing\":\n",
    "            print(\"The batch is completed and the results are being prepared.\")\n",
    "        elif status == \"completed\":\n",
    "            print(\"The batch is complete, downloading the results...\")\n",
    "            download_batch_results(batch_status.output_file_id, batch_number, directory)\n",
    "            return True\n",
    "        elif status == \"expired\":\n",
    "            print(\"The batch was not completed within the 24-hour time window.\")\n",
    "            return False\n",
    "        elif status == \"cancelling\":\n",
    "            print(\"The batch is being cancelled. Please wait...\")\n",
    "        elif status == \"cancelled\":\n",
    "            print(\"The batch was cancelled.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Unknown status encountered.\")\n",
    "\n",
    "        time.sleep(30)  # Wait for 30 seconds before checking the status again\n",
    "        \n",
    "def download_batch_results(output_file_id, batch_number, directory):\n",
    "    with open(os.path.join(directory, \"output_file_ids.txt\"), \"a\") as f:\n",
    "        f.write(output_file_id + \"\\n\")\n",
    "\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    \n",
    "    filename = f\"{directory}/batch_data_{batch_number}.jsonl\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(file_response.text)\n",
    "total_completions = 0\n",
    "batch_number = 0\n",
    "consecutive_failures = 0\n",
    "\n",
    "directory = os.path.join(\"data\", f\"batches_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "os.makedirs(os.path.join(directory, \"prompts\"), exist_ok=True)\n",
    "\n",
    "base_filename = os.path.join(directory, \"prompts\", \"batch\")\n",
    "batch_writer_iter = write_batch_completion_file(NUM_COMPLETIONS_PER_REQUEST, MODEL, base_filename, offset=OFFSET)\n",
    "with tqdm(total=NUM_COMPLETIONS, desc=\"Batch Generation\") as pbar:\n",
    "    while total_completions < NUM_COMPLETIONS and consecutive_failures < MAX_RETRIES:\n",
    "        try:\n",
    "            # 1. Write the batch completion file\n",
    "            batch_number += 1\n",
    "            filename = next(batch_writer_iter)\n",
    "\n",
    "            # 2. Upload the batch file\n",
    "            batch_input_file = client.files.create(\n",
    "                file=open(filename, \"rb\"),\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "            batch_input_file_id = batch_input_file.id\n",
    "            with open(os.path.join(directory, \"input_file_ids.txt\"), \"a\") as f:\n",
    "                f.write(batch_input_file_id + \"\\n\")\n",
    "\n",
    "            # 3. Create the batch job\n",
    "            batch_info = client.batches.create(\n",
    "                input_file_id=batch_input_file_id,\n",
    "                endpoint=\"/v1/chat/completions\",\n",
    "                completion_window=\"24h\",\n",
    "                metadata={\n",
    "                    \"description\": f\"Simple Stories Story Generation - batch {batch_number}, n={NUM_COMPLETIONS_PER_REQUEST}\"\n",
    "                }\n",
    "            )\n",
    "\n",
    "            batch_id = batch_info.id\n",
    "            with open(os.path.join(directory, \"batch_job_ids.txt\"), \"a\") as f:\n",
    "                f.write(batch_id + \"\\n\")\n",
    "\n",
    "            # 4. Check the status and download the results\n",
    "            if check_batch_status(batch_id, batch_number, directory):\n",
    "                total_completions += NUM_COMPLETIONS_PER_REQUEST\n",
    "                pbar.update(NUM_COMPLETIONS_PER_REQUEST)\n",
    "                consecutive_failures = 0\n",
    "            else:\n",
    "                consecutive_failures += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            consecutive_failures += 1\n",
    "\n",
    "    if consecutive_failures >= MAX_RETRIES:\n",
    "        print(f\"Stopping due to {MAX_RETRIES} consecutive failures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Format batch output\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade, dale_chall_readability_score\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "from generate_stories import process_completion\n",
    "\n",
    "def nlp_metrics(s: str) -> dict:\n",
    "    # Tokenize words and sentences\n",
    "    words = word_tokenize(s)\n",
    "    sentences = sent_tokenize(s)\n",
    "    \n",
    "    word_count = len(words)\n",
    "    character_count = len(s)\n",
    "    \n",
    "    avg_word_length = np.mean([len(word) for word in words]) if word_count > 0 else 0\n",
    "    avg_sentence_length = word_count / len(sentences) if sentences else 0\n",
    "    \n",
    "    flesch_reading = flesch_reading_ease(s)\n",
    "    flesch_kincaid = flesch_kincaid_grade(s)\n",
    "    dale_chall = dale_chall_readability_score(s)\n",
    "    \n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"character_count\": character_count,\n",
    "        \"avg_word_length\": round(avg_word_length, 2),\n",
    "        \"avg_sentence_length\": round(avg_sentence_length, 2),\n",
    "        \"flesch_reading_ease\": flesch_reading,\n",
    "        \"flesch_kincaid_grade\": flesch_kincaid,\n",
    "        \"dale_chall_readability_score\": dale_chall\n",
    "    }\n",
    "\n",
    "def format_jsonl(input_files, output_file):\n",
    "    assert not os.path.isfile(output_file), \"output file already exists\"\n",
    "\n",
    "    for k, input_file in enumerate(input_files):\n",
    "        with open(input_file, 'r') as infile, open(output_file, 'a') as outfile:\n",
    "            for line in infile:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                custom_id = data['custom_id']\n",
    "                match = re.search(r'{.*}', custom_id)\n",
    "                if match:\n",
    "                    params = json.loads(match.group(0))\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                completion = data['response']['body']['choices'][0]['message']['content']\n",
    "                gen_model = data['response']['body']['model']\n",
    "                \n",
    "                json_struct = process_completion(gen_model, completion, params, expected_num_stories=params.get(\"expected_num_in_stories\", None))\n",
    "                story_dicts = [item for item in json_struct if 'story' in item]\n",
    "                for k, story_dict in enumerate(story_dicts):\n",
    "                    story_dicts[k] = story_dict | nlp_metrics(story_dict['story'])\n",
    "                    \n",
    "                lines = [json.dumps(item) for item in story_dicts]\n",
    "                outfile.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "input_dir = os.path.join('data', 'batches_2024-11-12-17-00-38')\n",
    "input_files = []\n",
    "input_files.extend([os.path.join(input_dir,file) for file in os.listdir(input_dir) if file.endswith('.jsonl') and os.path.isfile(os.path.join(input_dir, file))])\n",
    "output_file = os.path.join(input_dir, 'processed.jsonl')\n",
    "\n",
    "format_jsonl(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Optionally convert to parquet and permute rows\n",
    "import pandas as pd\n",
    "\n",
    "parquet_file_path = output_file.replace(\".jsonl\", \".parquet\")\n",
    "\n",
    "df = pd.read_json(output_file, lines=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.to_parquet(parquet_file_path, engine='pyarrow', compression='snappy')\n",
    "df.to_json(output_file, orient='records', lines=True)\n",
    "\n",
    "# TODO: Maybe filter out excessively long completions, such as when the end string was not correctly generated. Also, check for non-Latin characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Proceed to either analyse_dataset.ipynb or embeddings.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
